\documentclass[12pt]{article}

\input{setup}

\newcommand{\subtask}[1]{{\bf #1.}}

\definecolor{Themecolor}{HTML}{C1DCBD}

\begin{document}
\begin{titlepage}
    \pagecolor{Themecolor}
    \newgeometry{inner=0pt, outer=0pt}
    \afterpage{
        \nopagecolor
        \restoregeometry
    }
    \begin{center}
        {\large\fontspec{Gill Sans}\addfontfeature{LetterSpace=20.0}STEFFEN HAUG}\\[2em]
        {\Huge\it Numerical Methods}
        \vfill
        {\ttfamily Assignment 1}
    \end{center}
\end{titlepage}

\begin{multicols}{2}

\section*{Problem 1}
Sphere {\em floating} in water, i. e. the sphere is still.
\begin{figure}[H]
\centering
\includestandalone[width=0.9\columnwidth]{kule}
\caption{
    Sphere in equilibrium.
}
\end{figure}
\noindent
The sphere has a density of $\rho = 0.6$ times that of water.
Archimedes' principle states that the buoyant force exerted
og the sphere is equal (in magnitude) to the {\em weight} of the displaced fluid.
    Using density $\rho_\text{fluid} = 1$ for water, the weight of the displaced fluid is
\[
    -\vec{B} = m_{\text{fluid}} \cdot g = v_{\text{fluid}} \cdot g \text,
\]
and we know that the weight of the sphere itself is
\[
    \vec{F} = m_{\text{sphere}} \cdot g = 0.6 \cdot v_{\text{sphere}} \cdot g \text.
\]
At equilibrium, these balance out:
\[
    0 = \vec F - \vec B
        = g(0.6v_\text{sphere} - v_\text{fluid})
\]
Obviously, the volume of displaced fluid is equal to the volume of the
part of the sphere that is submerged.
We divide the equation by $g$, and thus
\begin{equation}
    v_\text{submerged} = v_\text{fluid} = 0.6 \cdot v_\text{sphere}
\end{equation}
we also know that
\begin{equation}
    v_\text{sphere} = \frac{4\pi r^3}{3}
\end{equation}
\begin{figure}[H]
\centering
\includestandalone[width=0.9\columnwidth]{sub_vol}
\caption{
    Volume of the submerged part of the sphere.
}
\end{figure}
\noindent
and with some analysis, we can work out the volume of the submerged
part of the sphere. Our strategy is to integrate over
cylinder-shaped volume elements.

Each volume element has a volume of $\pi \cdot R(r^*)^2 \cdot \D r^*$,
and we need to integrate from water level and downward in a
coordinate system on the sphere.
Thus,
\begin{equation}
    v_\text{submerged} =
        \pi \int\limits_{\mathclap{r^* = -x^*}}^{\mathclap{r^* = r}}R(r^*)^2 \D r^*
\end{equation}
where
\[
    R(r^*) = \sqrt{r^2 - (r^*)^2} \,\text.
\]
We now have our mathematical model.
It remains to find a solution $x^*$ such that
\[
    I(x^*) \coloneqq \frac{v_\text{submerged}}{v_\text{sphere}}
    = \frac{3}{4 \pi r^3}
        \pi \int\limits_{\mathclap{r^* = -x^*}}^{\mathclap{r^* = r}}R(r^*)^2 \D r^*
    = \rho = 0.6
\]
Given an algorithm to compute definite integrals, this can probably already be
solved quite easily, but we don't know any such algorithms yet,
so our strategy is to evaluate the integral by hand, and try
to create a root-finding problem.
\begin{align*}
    I(x^*) &= \frac{3}{4 r^3}
        \int\limits_{\mathclap{r^* = -x^*}}^{\mathclap{r^* = r}}R(r^*)^2 \D r^* \\
    &= \frac{3}{4 r^3}
        \int\limits_{\mathclap{r^* = -x^*}}^{\mathclap{r^* = r}}
            r^2 - (r^*)^2 \D r^* \\
    &= \frac{3}{4 r^3} \cdot
    \smashoperator{\Sub\limits_{r^* = -x^*}^{r^* = r}}r^2 r^* - \frac{1}{3}(r^*)^3 \\
    &= \frac{1}{4 r^3} \left(
    3r^2(r + x^*) - r^3 - (x^*)^3
    \right)
\end{align*}
This is a good time to check that
\[
        I(-r) = 0\text, \quad
        I(0) = 1/2\text, \quad \text{and} \quad
        I(r) = 1
\]
Which is our limit cases, where the sphere is either hovering
above the fluid ($x^* = -r$), or is completely submerged ($x^* = r$).
The $x^* = 0$ case corresponds to exactly half the sphere
being submerged.
\begin{lstlisting}[language=Python, caption={Checking for a given radius}]
def I(h, r):
    p = (3 * r**2 * (r + h) - r**3 - h**3)
    q = (4 * r**3)
    return p / q
\end{lstlisting}
\vspace{-0.5\baselineskip}
\begin{lstlisting}[language=Python, frame=b,numbers=none]
>>> r = 17
>>> I(r, r)
1.0
>>> I(-r, r)
0.0
>>> I(0, r)
0.5
\end{lstlisting}
All that remains is to solve the equation
\[
    I(x^*) - \rho = 0\text; \quad 0 < \rho < 1 \,\text,
\]
where $\rho$ is the density of the sphere.
Our analysis reveals that $I(x^*)$ is a polynomial of degree three,
which is good because its image is all of $\R$, so a root like this is guaranteed
to exist. {\em Unfortunately, three roots exist.}
We need to define a criteria to determine which solution is the correct one.

Obciously, the sphere will not hover in mid-air, and since we
have assumed $\rho < 1$, it will not sink.
Clearly then, all {\em physical} solutions must be in the interval
$[-r, r]$.
If we were to prove that the {\sc contraction mapping theorem}
applies to some function $g: [-r, r] \rightarrow [-r, r]$, then
we would have a guarantee that the physical solution is unique,
and we would have defined a procedure to approximate it
in the proccess.

\subsection*{Applying the contraction\\ mapping theorem}
If we factor $I(x^*)$ in a particular manner, we see that
\[
    I(x^*) = \frac{x^*}{4r} \left(
    3 - \left(\frac{x^*}{r}\right)^2
    \right) + \frac{1}{2} \,\text,
\]
and that we may write the equation $I(x^*) - \rho = 0$ as
\[
    x^* = g(x^*) =
    -4r
    \left(
    \frac{1 - 2\rho}{2}
    \right)
    \left(
    3 - \left(\frac{x^*}{r}\right)^2
    \right)^{-1}
    \text.
\]
Notice that $g(x^*)$ is continuous on $[-r, r]$.
It does have singularities at $x^* = \pm r \sqrt 3$,
but that is outside the interval,
hence the mean value theorem holds:
\[
    g(x) - g(y) = g'(m)(x - y) \,\text,
\]
for some $m \in [-r, r]$.
To place a bound on $g'(m)$, we need to evaluate
$\textdd{g}{x^*}$ using the chain rule.
\begin{align*}
    \dd{g}{x^*} &=
    \underbrace{
    -4r
    \left(
    \frac{1 - 2\rho}{2}
    \right)}_\text{constant}
    \underbrace{\dd{}{x^*}
    \left(3 - \left(\frac{x^*}{r}\right)^2\right)^{-1}
    }_{\gamma(x^*)} \\
    \gamma &=
    -\left(3 - \left(\frac{x^*}{r}\right)^2\right)^{-2}
    \dd{}{x^*}
    -\left(\frac{x^*}{r}\right)^2 \\
    &=
    \left(3 - \left(\frac{x^*}{r}\right)^2\right)^{-2}
    2\left(\frac{x^*}{r}\right)
    \dd{}{x^*}
    \frac{x^*}{r} \\
    &=
    \left(3 - \left(\frac{x^*}{r}\right)^2\right)^{-2}
    2\left(\frac{x^*}{r}\right)
    \frac{1}{r}
\end{align*}
We can rearrange this a little, and multiply by $1 = r^2 \cdot r^{-2}$:
\begin{align*}
    \gamma &=
    2 x^* r^{-2} \left(3 - \left(\frac{x^*}{r}\right)^2\right)^{-2}
    \cdot r^2r^{-2} \\
    &=
    2 x^* r^2 \left(3r^2 - \left(\frac{x^*}{r}\right)^2r^2\right)^{-2} \\
    &=
    2 x^* r^2 \left(3r^2 - \left(x^*\right)^2\right)^{-2}
\end{align*}
then
\begin{align*}
    \dd{g}{x^*} &=
    -4r
    \left(
    \frac{1 - 2\rho}{2}
    \right) \gamma(x^*) \\
    &=
    \underbrace{
    -8r^3
    \left(
    \frac{1 - 2\rho}{2}
    \right) x^*}
_\text{monotonically decreasing}
    \overbrace{\left(3r^2 - \left(x^*\right)^2\right)^{-2}}
    ^{\mathclap{\text{strictly positive for } x^* \in [-r, r]}}
\end{align*}
i. e., $g'(x^*)$ is itself monotonically decreasing.
This is very fortunate, since it suffices to evaluate
$g'$ in the endpoints of $[-r, r]$.
\begin{align*}
    \smashoperator[r]{\Sub \limits_{x^* = -r}} \dd{g}{x^*} &=
    -\frac{1 - 2\rho}{2}\frac{8r^4}{(2r^2)^2} = 2\rho - 1 \\
    \smashoperator[r]{\Sub \limits_{x^* = r}} \dd{g}{x^*} &=
    \frac{1 - 2\rho}{2}\frac{8r^4}{(2r^2)^2} = -(2\rho - 1)
\end{align*}
This finally proves that
\[
    \smashoperator[l]{\max_{x^* \in [-r, r]}} \left|\dd{g}{x^*}\right| = |2\rho - 1|
    \,\text,
\]
and moreover, $|2\rho - 1| < 1$ by the assumption ${0 < \rho < 1}$.
This works out exactly so that the assumptions of the contraction mapping theorem holds:
\begin{align*}
    |g(x) - g(y)| &= |g'(m)||x - y|
    \tag*{\footnotesize\scshape mean value theorem} \\
    &\leq |2\rho - 1||x - y| \\
    &< |x - y|
\end{align*}
In other words, the contraction mapping theorem
holds with $L = 1$.
Thus
\[
    \left\{\begin{array}{l}
        \big\{ x_n \big\}_{n = 0}^{\infty} \\
        x_n = g \left( x_{n-1} \right) \\
        x_0 \in [-r, r]
    \end{array}\right.
\]
converges to the unique, physical solution, no matter the choice of $x_0$.


\section*{Problem 2}
\subtask{a} Want to use the Taylor series of $f$ around the point $a$,
$\Tay{f}{a}{x}$, to show that

\[
    \dd{f}{x} = \frac{f(x + \Delta x) - f(x)}{\Delta x} + {\mathcal O}(\Delta x),
\]
in other words, that the error we expect if we compute
\[
    \dd{f}{x} \approx \frac{f(x + \Delta x) - f(x)}{\Delta x},
\]
is proporional to $\Delta x$, i. e. the error goes to zero
as $\Delta x$ approaches zero.

Fix $x$ and $\Delta x$. We expand the Taylor series to two terms,
{\em also including the error term}, around the point $x$.
\begin{align*}
    f(x^*) &= \Tay{f}{x}{x^*} \\
    &= f(x) + f'(x)(x^* - x) + \Err{2}(x^*) \\
    \intertext{Which yields}
    f'(x) &= \frac{f(x^*) - f(x) - \Err{2}(x^*)}{x^* - x}
    \intertext{By evaluating the series at $x^* = x + \Delta x$}
    f'(x) &= \frac{f(x + \Delta x) - f(x)}{\Delta x}
        - \frac{\Err{2}(x^*)}{\Delta x}
\end{align*}
To show that the error term is $\mathcal O(\Delta x)$ is straight forward:
\begin{align*}
    E(\Delta x) \coloneqq \frac{\Err{2}(x^*)}{\Delta x}
        &= \frac{1}{\Delta x} \frac{f''(\widetilde{x})}{2!}(x^* - x)^2 \\
    \intertext{for some $\widetilde{x} \in [x, x^*]$. Again, we evaluate
    the error term at $x^* = x + \Delta x$.}
    &= \frac{f''(\widetilde{x})}{2 \Delta x}(\Delta x)^2 \\
    &= \frac{f''(\widetilde{x})}{2} \Delta x
\end{align*}
as long as $f''$ does not blow up on $I = [x, x + \Delta x]$.
Because of the fact that $I$ is bounded and closed, $f''$ attains its maximum and
minimum on $I$. Thus we can let
\[
    L = \Diam\;f(I) = \max_{x, y \;\in\; I} \left|f''(x) - f''(y)\right| \\
\]
Which makes
\[
    |E(\Delta x)| \leq L \Delta x \,\text.
\]

\subsection*{Testing the approximation}
We want to compute the error commited by our formula for
some function which we know how to differentiate symbolically.
We want to compute
\[
    \Sub\limits_{\mathclap{x = \pi/4}} \dd{}{x} \cos x \,\text.
\]
We know that the symbolic derivative of $\cos x$ is $-\sin x$.

% source code
\begin{lstlisting}[language=Python, caption={Approximate differentiation program}]
# src/alg.py
def ddx(f, dx):
    return lambda x: (f(x + dx) - f(x)) / dx
\end{lstlisting}
{\ttfamily ddx} returns a function
of type {\ttfamily float -> float}
that closes over {\ttfamily f} and {\ttfamily dx}.
This new function in turn evaluates the
derivative at a point {\ttfamily x}.

\begin{lstlisting}[language=Python, caption={Using the differentiation program}]
import src.alg as alg
import numpy as np
from math import sin, cos, pi

def errors(deltas):
    for dx in deltas:
        dcosdx = alg.ddx(cos, dx)
        approx = dcosdx(pi/4)
        exact  = -sin(pi/4)
        yield (abs(approx - exact), dx)

dxs = (1/2**k for k in range(30))
data = list(errors(dxs))
error, delta = zip(*data)
\end{lstlisting}
Plotting {\ttfamily error} against {\ttfamily delta} in
a logarithmic coordinate system confirms that the error
is indeed linear in $\Delta x$.

\begin{figure}[H]
\centering
\includestandalone[width=0.6\columnwidth]{ddx}
\caption{
    The absolute error of the approximation as a function
    of $\Delta x$ for $f(x) = \cos x$ and $x = \pi/4$
}
\end{figure}
\noindent



\noindent\subtask{b} We want to observe the propagation of numerical
error when iterating with a transformation using floating-point
arithmetic.
\begin{lstlisting}[language=Python, caption={{Iterating with} {\ttfamily sqrt}}]
from math import sqrt
import random as rand

def iter_sqrt(N):
    x  = rand.random() * 100
    xh = x

    for l in range(1, N):
        for k in range(1, l):
            xh = sqrt(xh)
        for k in range(1, l):
            xh = xh**2

        e  = abs(x - xh)
        er = abs(x - xh) / abs(x)

        yield (e, er, l)

data = list(iter_sqrt(N = 100))
es, ers, ls = zip(*data)
\end{lstlisting}

\begin{figure}[H]
\centering
\includestandalone[width=0.6\columnwidth]{sqrt}
\caption{
    The absolute and relative error committed by the algorithm.
}
\end{figure}
\noindent

\section*{Problem 3}
\begin{lstlisting}[language=Python, caption={Bisection and Newtons method}]
MAX_ITER = 100

def bisect(f, I):
    a, b = I

    assert a < b
    assert f(a) * f(b) < 0

    for _ in range(MAX_ITER):
        m = 0.5 * (a + b)
        yield m
        if f(a) * f(m) < 0:
            b = m
        elif f(b) * f(m) < 0:
            a = m
        else: return # then f(m) == 0, so stop


def fpi(g, x0):
    xn    = g(x0) # x1
    xn_m1 = x0    # x0
    for _ in range(MAX_ITER):
        yield xn
        xn, xn_m1 = g(xn), xn


def newton(f, dfdx, x0):
    g = lambda x: x - f(x) / dfdx(x)
    yield from fpi(g, x0)

def pairs(it):
    x = next(it)
    y = next(it)
    while True:
        yield(x, y)
        x, y = y, next(it)
\end{lstlisting}
As you can see, we can implement Newtons method
as a generic fixed-point iteration.
I have opted for differentiating $f$ symbolically
and providing it to {\ttfamily newton}.

The functions are defined in a way, such that they
yield consecutively better approximations, instead
of returning the best approximation in the end.
This is just because we want to illustrate the convergence.
Moreover, we are interested in two types of convergence,
which the following figures show.

Plotting the errors versus {\ttfamily i} and {\ttfamily j},
which are the iteration count,
we see clearly that Newtons method converges quadratically,
while bisection converges linearly.

\begin{lstlisting}[language=Python, caption={Comparing the two methods}]
def g(x): return x**5 - 4*x + 2
def dgdx(x): return 5*x**4 - 4

n = list(alg.newton(g, dgdx, 1))
b = list(alg.bisect(g, (1, 5)))

def abs_err(data):
    for xn, xn_p1 in alg.pairs(iter(data)):
        yield abs(xn - xn_p1)

i, err_abs_n = zip(*list(enumerate(abs_err(n))))
_, err_abs_b = zip(*list(enumerate(abs_err(b))))

def f_err(f, data):
    for xn in data:
        yield abs(f(xn))

j, err_f_n = zip(*list(enumerate(f_err(g, n))))
_, err_f_b = zip(*list(enumerate(f_err(g, b))))
\end{lstlisting}
\begin{figure}[H]
\centering
\includestandalone[width=0.7\columnwidth]{conv}
\caption{
    Speed of convergence $|f(x_k)|$.
    This clearly illustrates that the midpoint moves back and forth,
    as the interval shrinks from different sides.
}
\end{figure}
\noindent

\begin{figure}[H]
\centering
\includestandalone[width=0.7\columnwidth]{conv2}
\caption{
    Speed of convergence $|x_{k+1} - x_k|$.
}
\end{figure}

\subsection*{Numerical experiment}
We want to verify that bisection converges linearly,
and that Newton's method converges quadratically, by
a numerical experiment.
Recalling the definition of speed of convergence,
we need to find two sequences, for bisection $x_n$:
\[
    \Seq{\epsilon _n}_{n = 0}^\infty \;\text;\quad
        \lim_{n \rightarrow \infty}
            \frac
                {\left| \epsilon_{n + 1} \right|}
                {\left| \epsilon_n \right|}
        = \mu \,\text,
\]
and for Newtons method $y_n$:
\[
    \Seq{\delta_n}_{n = 0}^\infty \;\text;\quad
        \lim_{n \rightarrow \infty}
            \frac
                {\left| \delta_{n + 1} \right|}
                {\left| \delta_n \right|^2}
        = \nu \,\text.
\]
This measure of the error is slightly different from the
ones we plotted before, so some programming work is required:
\begin{lstlisting}[language=Python, caption={Generators for the errors}]
ns = list(alg.newton(g, dgdx, 1))
bs = list(alg.bisect(g, (1, 5)))

# define the limit as the last
# iteration of newtons method.
L = ns[-1]

ns_err = (abs(xn - L) for xn in ns)
bs_err = (abs(xn - L) for xn in bs)
\end{lstlisting}

\subsection*{Bounding the error}
Our strategy is to try and come up with two sequences of
the kind above, such that one bounds
the error from above, and one bounds the error from below.
If we can find two such sequences where the inequality holds
for any number of iterations, then certainly the
sequence in-between converges at the same rate.
There are two problems with this: First and foremost
{\em we only have a finite number of terms of the sequence},
so we can't say much about the asymptotic behaviour.
Secondly, {\em we only have finite precision},
so at some point, small numbers are rounded down to zero,
which means that even if we {\em could} compute
an infinite number of elements, the asymptotic behaviour would
be incorrect past some point.

We begin with the bisection method, as the linear case is (presumably)
simpler.
The obvious sequence to compare the error with is
\[
    \Seq{\epsilon_n}(x) = \left\{\begin{array}{l@{\,}l}
        \epsilon_{n+1} & = \mu \cdot \epsilon_n \\
        \epsilon_0 & = x
    \end{array}\right.
\]
Where we ``guess'' $\mu \coloneqq 1/2$, because each iteration
makes the interval exactly half the size.
Notice that $\epsilon_n$ can be computed
with an iterative procedure using the function $x \mapsto \mu x$.
\begin{lstlisting}[language=Python, caption={Computing the sequences}]
mu = 0.5
es1 = list(alg.fpi(lambda x: mu * x, 10))
es2 = list(alg.fpi(lambda x: mu * x, 10E-4))
\end{lstlisting}
\begin{figure}[H]
\centering
\includestandalone[width=0.7\columnwidth]{exp_bisect}
\caption{
    Absolute error of bisection method, with the
    sequence $\epsilon_n$.
    Past roughly $10^{-15}$, the error ``drops'' suddenly.
}
\end{figure} \noindent
The sudden drop in error may be explained by
the fact that the bisection algorithm needs to
evaluate $f(x)$ in the endpoints, and
as we approach the solution, this value becomes
incredibly small.
If we come {\em close enough}, this must be rounded down
to zero, because python does not do arbitrary precision
arithmetic.

For Newton's method, the choice of constants is less clear.
Never the less, we are looking for a sequence
\[
    \Seq{\delta_n}(x) = \left\{\begin{array}{l@{\,}l}
        \delta_{n+1} & = \nu \cdot \delta_n^2 \\
        \delta_0 & = x
    \end{array}\right.
\]
I have set $\nu \coloneqq 9/10$, which i found by trial and error.
Presumably, it is possible to find a constant by doing
some analysis.

We iterate with $x \mapsto \nu \cdot x^2$,
and we see the same thing happen here: Errors smaller than $10^{-15}$
or so are rounded down to zero, so it is really hard to see
if the error is {\em really} bounded by the sequences.
Not only that, but Newtons method converges so quickly that
we can only consider the first seven iterations before the error is
rounded down.


\begin{figure}[H]
\centering
\includestandalone[width=0.7\columnwidth]{exp_newton}
\caption{
    Absolute error of newtons method,
    with the sequence $\delta_n$.
}
\end{figure}

\end{multicols}
\end{document}
